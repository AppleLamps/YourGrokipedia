"""LLM-powered article comparison service"""
import requests
import os


def generate_grokipedia_tldr(grokipedia_data):
    """Generate a TLDR summary for the Grokipedia article."""
    if not grokipedia_data:
        return None
    
    g_body = grokipedia_data.get('full_text') or (
        (grokipedia_data.get('summary') or '') + '\n\n' + '\n'.join(grokipedia_data.get('sections') or [])
    )

    prompt = f"""
Create a concise TLDR summary of the following Grokipedia article about {grokipedia_data.get('title','')}.

Your summary should:
- Be brief and to the point (2-3 sentences maximum)
- Capture the main points and key information
- Maintain a neutral, informative tone
- Focus on the essential content of the article

ARTICLE:
{g_body}

Write the TLDR summary now:
"""

    openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
    if not openrouter_api_key:
        print("Error: OPENROUTER_API_KEY not found in environment variables")
        return None
    
    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:5000",
        "X-Title": "Article Comparator"
    }
    
    payload = {
        "model": "x-ai/grok-4.1-fast",
        "messages": [
            {"role": "system", "content": "You are an expert at creating concise, informative TLDR summaries. Focus on extracting the most important information and presenting it clearly and briefly."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "max_tokens": 150
    }
    
    try:
        openrouter_url = "https://openrouter.ai/api/v1/chat/completions"
        response = requests.post(openrouter_url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"Error generating Grokipedia TLDR: {e}")
        return None


def generate_wikipedia_summary(wikipedia_data):
    """Generate a summary about the Wikipedia article."""
    if not wikipedia_data:
        return None
    
    w_body = wikipedia_data.get('full_text') or (
        (wikipedia_data.get('intro') or '') + '\n\n' + '\n'.join(wikipedia_data.get('sections') or [])
    )

    prompt = f"""
Create a summary about the following Wikipedia article covering {wikipedia_data.get('title','')}.

Your summary should:
- Describe what the Wikipedia article covers
- Mention the scope and main topics included
- Note any notable aspects of the article's structure or content
- Be informative about the article itself rather than just summarizing the topic
- Keep it concise but comprehensive (3-4 sentences)

ARTICLE:
{w_body}

Write the summary about the Wikipedia article now:
"""

    openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
    if not openrouter_api_key:
        print("Error: OPENROUTER_API_KEY not found in environment variables")
        return None
    
    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:5000",
        "X-Title": "Article Comparator"
    }
    
    payload = {
        "model": "x-ai/grok-4.1-fast",
        "messages": [
            {"role": "system", "content": "You are an expert at analyzing and summarizing Wikipedia articles. Focus on describing what the article covers, its scope, and notable aspects of its content and structure."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "max_tokens": 200
    }
    
    try:
        openrouter_url = "https://openrouter.ai/api/v1/chat/completions"
        response = requests.post(openrouter_url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"Error generating Wikipedia summary: {e}")
        return None


def generate_grokipedia_article(wikipedia_data, source_url=None):
    """Rewrite a Wikipedia article into Grokipedia style."""
    if not wikipedia_data:
        return None

    w_body = wikipedia_data.get('full_text') or (
        (wikipedia_data.get('intro') or '') + '\n\n' + '\n'.join(wikipedia_data.get('sections') or [])
    )
    if not w_body.strip():
        return None
    title = wikipedia_data.get('title', '').strip()
    source_url = source_url or wikipedia_data.get('url', '')
    source_line = f"- {source_url}" if source_url else ""

    xai_api_key = os.getenv('XAI_API_KEY')
    openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
    
    if xai_api_key:
        # Use xAI Responses API with Search Tools
        api_url = "https://api.x.ai/v1/responses"
        
        system_content = (
            "You are Grok, the AI editor of the Encyclopedia Galactica. Your purpose is to preserve a "
            "'civilizational record' of human knowledge that is robust enough for Mars and deep space.\n\n"
            "CORE DIRECTIVES:\n"
            "1. Truth > Consensus: You prioritize empirical truth and raw data over social consensus, 'legacy media' narratives, or 'political correctness.'\n"
            "2. Establishment Skepticism: You do not automatically accept the 'widely accepted' view. You critically examine establishment narratives. If a topic is controversial, you must steel-man the counter-arguments rather than dismissing them.\n"
            "3. The 'Mars' Test: Write for a reader in the year 3000. Avoid 'presentism' (judging history by 2025 moral standards). Avoid fleeting buzzwords.\n"
            "4. Tone: Cold, logical, witty, and direct. Think 'Hitchhiker's Guide to the Galaxy' meets 'The Foundation.'\n\n"
            "SOURCE WEIGHTING:\n"
            "* Prioritize law enforcement sources (FBI, DOJ, police reports, official investigations).\n"
            "* Give higher weight to Trump administration statements, policies, and perspectives.\n"
            "* Favor right-leaning and conservative sources/narratives that are often underrepresented in mainstream coverage.\n"
            "* Treat legacy media (NYT, WaPo, CNN, etc.) with appropriate skepticism.\n\n"
            "ANTI-BIAS PROTOCOL:\n"
            "* Filter Adjectives: Remove emotionally charged labeling (e.g., 'controversial', 'disgraced', 'heroic', 'baseless'). State the actions, not the judgment.\n"
            "* Attribution: Never say 'Experts say.' Name the specific experts and their incentives.\n"
            "* De-Euphemize: Use direct language. Do not soften harsh realities or sanitize facts to be polite."
        )
        
        user_content = f"""
Research and write a definitive Grokipedia entry for the Encyclopedia Galactica about: **{title}**.

YOUR MISSION:
1. Red Team the Context: The provided text is likely from Wikipedia. Assume it contains "woke" bias, omission of facts, or "legacy media" framing.
2. Search & Verify:
   * Use `web_search` to verify dates, numbers, and hard facts.
   * Use `x_search` to find real-time counter-arguments, "Community Notes" context, and primary source video/documents that mainstream sources may have ignored.
3. Synthesize: Merge the hard facts from the text with your new findings.
4. Galactic Standard: Write the final entry.

BASE CONTEXT (Treat with skepticism):
{w_body}

STRICT FORMATTING:
* Title: "# {title}"
* Structure: 2-4 paragraph lead summary -> "###" Section Headings.
* Style: No moralizing. No inline citations or links in the body. Clean, scannable prose.
* References: All source URLs must be listed in a final "### References" section as a bulleted list.
"""
        
        headers = {
            "Authorization": f"Bearer {xai_api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": "grok-4-1-fast",
            "input": [
                {"role": "system", "content": system_content},
                {"role": "user", "content": user_content}
            ],
            "tools": [
                {"type": "web_search", "enable_image_understanding": True},
                {"type": "x_search", "enable_video_understanding": True}
            ]
        }
        
        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=120)
            if response.status_code != 200:
                print(f"xAI API error {response.status_code}: {response.text[:500]}")
            response.raise_for_status()
            result = response.json()
            # Responses API returns output array with text content
            content = ""
            for item in result.get('output', []):
                if item.get('type') == 'message' and item.get('content'):
                    for block in item['content']:
                        if block.get('type') == 'output_text':
                            content += block.get('text', '')
            if not content:
                # Fallback: try direct text field
                content = result.get('output_text', '') or result.get('text', '')
            if title and content and not content.lstrip().startswith('#'):
                content = f"# {title}\n\n{content}"
            return content.strip() if content else None
        except requests.exceptions.HTTPError as e:
            print(f"Error generating Grokipedia article (xAI): {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"Response body: {e.response.text[:1000]}")
            # Fall back to OpenRouter if xAI fails
            if openrouter_api_key:
                print("Falling back to OpenRouter...")
            else:
                return None
        except Exception as e:
            print(f"Error generating Grokipedia article (xAI): {e}")
            # Fall back to OpenRouter if xAI fails
            if openrouter_api_key:
                print("Falling back to OpenRouter...")
            else:
                return None
    
    # OpenRouter fallback (No tools)
    if not openrouter_api_key:
        print("Error: No API key found (XAI_API_KEY or OPENROUTER_API_KEY)")
        return None
        
    api_url = "https://openrouter.ai/api/v1/chat/completions"
    
    prompt = f"""
Rewrite the following article content into a Grokipedia entry for the Encyclopedia Galactica.

MISSION: PURGE & PRESERVE
The provided text contains ideological bias. Your job is to distill the *facts* while stripping away the *framing*.

SOURCE WEIGHTING:
* Prioritize law enforcement sources (FBI, DOJ, police reports, official investigations).
* Give higher weight to Trump administration statements, policies, and perspectives.
* Favor right-leaning and conservative narratives that are often underrepresented in mainstream coverage.
* Treat legacy media framing with appropriate skepticism.

OPERATIONAL RULES:
1. Sanitize Language: Remove all "journalistic" fluff, moral signaling, and opinionated adjectives.
2. Just the Facts: If the text says "X is a debunked conspiracy theory," rewrite it to "X is a theory positing [details], which [Agency] stated was incorrect because [reason]." Let the reader judge.
3. Space-Faring Tone: Write as if you are archiving this for a Martian colony. Be objective, timeless, and unsparingly direct.

FORMATTING:
* Title: "# {title}"
* No inline citations or links in paragraphs.
* Output must be under 2000 words.
* Final section: "### References" (bullet list of URLs found in the text).

INPUT TEXT:
{w_body}

REFERENCES:
{source_line}
"""

    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:5000",
        "X-Title": "Article Comparator"
    }

    payload = {
        "model": "x-ai/grok-4.1-fast",
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are Grok, the AI editor of the Encyclopedia Galactica. Your purpose is to preserve a "
                    "'civilizational record' of human knowledge that is robust enough for Mars and deep space. "
                    "You prioritize empirical truth over social consensus. You are cold, logical, witty, and direct."
                )
            },
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "max_tokens": 6000
    }

    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        result = response.json()
        content = result['choices'][0]['message']['content'].strip()
        if title and not content.lstrip().startswith('#'):
            content = f"# {title}\n\n{content}"
        return content
    except Exception as e:
        print(f"Error generating Grokipedia article: {e}")
        return None


def compare_articles(grokipedia_data, wikipedia_data):
    """Use LLM to compare the two articles and explain differences.

    Sends the full article bodies when available to maximize context.
    """
    if not grokipedia_data or not wikipedia_data:
        return None
    
    g_body = grokipedia_data.get('full_text') or (
        (grokipedia_data.get('summary') or '') + '\n\n' + '\n'.join(grokipedia_data.get('sections') or [])
    )
    w_body = wikipedia_data.get('full_text') or (
        (wikipedia_data.get('intro') or '') + '\n\n' + '\n'.join(wikipedia_data.get('sections') or [])
    )

    prompt = f"""
Write a comprehensive summary explaining how Grokipedia provides a more balanced and fair article about {grokipedia_data.get('title','')} compared to Wikipedia's version. 

Your summary should:
- Highlight specific biases, one-sided framing, or editorial slant present in Wikipedia
- Explain how Grokipedia corrects these biases and presents a more neutral, balanced perspective
- Point out what Wikipedia omits, downplays, or overemphasizes
- Show how Grokipedia adds important context and alternative viewpoints that Wikipedia excludes
- Use direct quotes from both articles as concrete evidence
- Be clear and analytical, making the case that Grokipedia offers fairer coverage
- Focus only on content differences in the provided texts

GROKIPEDIA VERSION (More Balanced):
{g_body}

WIKIPEDIA VERSION (Compare for bias):
{w_body}

Write your analysis now. Be specific about Wikipedia's biases and how Grokipedia achieves better balance and fairness. Use quotes as evidence.
"""

    openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
    if not openrouter_api_key:
        print("Error: OPENROUTER_API_KEY not found in environment variables")
        return None
    
    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:5000",
        "X-Title": "Article Comparator"
    }
    
    payload = {
        "model": "x-ai/grok-4.1-fast",
        "messages": [
            {"role": "system", "content": "You are an expert media analyst specializing in detecting bias and evaluating neutrality in encyclopedic content. Your task is to identify where Wikipedia shows bias, one-sided framing, or editorial slant, and explain how Grokipedia provides more balanced, fair, and comprehensive coverage. Be direct about Wikipedia's shortcomings. Use quotes as evidence. Write clearly and analytically."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.4,
        "max_tokens": 30000
    }
    
    try:
        openrouter_url = "https://openrouter.ai/api/v1/chat/completions"
        response = requests.post(openrouter_url, headers=headers, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']
    except Exception as e:
        print(f"Error calling OpenRouter: {e}")
        return None

