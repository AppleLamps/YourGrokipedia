"""Article fetching services for Wikipedia and Grokipedia"""
import logging
import os
import re
from typing import Any, Optional

import requests
from urllib.parse import urlparse

from app.utils.url_parser import extract_article_title
from app.utils.sdk_manager import get_sdk_client, is_sdk_available, ArticleNotFound, RequestError

logger = logging.getLogger(__name__)

# Module-level session for connection pooling
_session: Optional[requests.Session] = None


def _get_session() -> requests.Session:
    """Get or create a shared requests session for connection pooling."""
    global _session
    if _session is None:
        _session = requests.Session()
        _session.headers.update({
            'User-Agent': 'Grokipedia-Comparator/1.0 (contact: example@example.com)'
        })
    return _session

# API timeouts (seconds)
DEFAULT_TIMEOUT = 30
FIRECRAWL_TIMEOUT = 60

# Firecrawl API configuration
FIRECRAWL_API_KEY = os.getenv('FIRECRAWL_API_KEY')
FIRECRAWL_API_URL = "https://api.firecrawl.dev/v1/scrape"

FIRECRAWL_CHROME_LINES = {
    "search",
    "suggest article",
    "edits history",
    "edit history",
    "new search",
    "sign in",
    "log in",
    "login",
    "logout",
    "log out",
    "home",
    "menu"
}

FIRECRAWL_SHORTCUT_LINES = {
    "cmd+k",
    "command+k",
    "command + k",
    "ctrl+k",
    "ctrl + k",
    "ctrl k",
    "cmd k",
    "\u2318k",
    "\u2318 k"
}

FOOTNOTE_DEFINITION_RE = re.compile(r'^\[\d+\]:\s*\S+', re.IGNORECASE)
FACT_CHECK_LINE_RE = re.compile(r'^fact-checked by\b', re.IGNORECASE)


def clean_firecrawl_markdown(markdown: str, title: str = "") -> str:
    """Strip UI chrome and clean Firecrawl markdown for display."""
    if not markdown:
        return markdown

    markdown = re.sub(r'\\+([\[\]()]|\\)', r'\1', markdown)

    title_normalized = (title or "").strip().lower()
    cleaned_lines = []
    blank_count = 0

    for raw in markdown.splitlines():
        stripped = raw.strip()
        if not stripped:
            blank_count += 1
            if blank_count <= 2:
                cleaned_lines.append('')
            continue

        blank_count = 0
        normalized = re.sub(r'\s+', ' ', stripped).lower()

        if normalized in FIRECRAWL_CHROME_LINES:
            continue
        if normalized in FIRECRAWL_SHORTCUT_LINES:
            continue
        if FACT_CHECK_LINE_RE.match(normalized):
            continue
        if FOOTNOTE_DEFINITION_RE.match(stripped):
            continue

        if title_normalized and normalized == title_normalized and not stripped.startswith('#'):
            continue

        cleaned_lines.append(stripped)

    return '\n'.join(cleaned_lines).strip()


def scrape_with_firecrawl(url: str) -> Optional[dict[str, Any]]:
    """Scrape a URL using Firecrawl API and return clean markdown."""
    if not FIRECRAWL_API_KEY:
        return None
    try:
        payload = {
            "url": url,
            "onlyMainContent": True,
            "formats": ["markdown"]
        }
        
        headers = {
            "Authorization": f"Bearer {FIRECRAWL_API_KEY}",
            "Content-Type": "application/json"
        }
        
        response = _get_session().post(FIRECRAWL_API_URL, json=payload, headers=headers, timeout=FIRECRAWL_TIMEOUT)
        response.raise_for_status()
        
        data = response.json()
        if data.get('success') and data.get('data'):
            return {
                'markdown': data['data'].get('markdown', ''),
                'title': data['data'].get('metadata', {}).get('title', ''),
                'description': data['data'].get('metadata', {}).get('description', ''),
                'url': url
            }
        return None
    except requests.RequestException as e:
        logger.warning("Firecrawl request failed: %s", e)
        return None


def scrape_wikipedia(url: str) -> Optional[dict[str, Any]]:
    """Fetch content from Wikipedia using official APIs for reliability.

    Returns dict with title, intro, sections (list[str]), url, and full_text (entire plaintext article).
    """
    try:
        # Extract the page title from URL
        title = extract_article_title(url)
        if not title:
            return None

        # 1) Fetch summary via REST API
        summary_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{title}"
        summary_resp = _get_session().get(summary_url, timeout=DEFAULT_TIMEOUT)
        if summary_resp.status_code == 404:
            return None
        summary_resp.raise_for_status()
        summary_data = summary_resp.json()
        title_text = summary_data.get('title') or title.replace('_', ' ')
        intro_text = summary_data.get('extract', '').strip()

        # 2) Fetch sections via Action API
        sections_url = (
            "https://en.wikipedia.org/w/api.php?action=parse&prop=sections&format=json&page="
            + title
        )
        sections_resp = _get_session().get(sections_url, timeout=DEFAULT_TIMEOUT)
        sections = []
        if sections_resp.ok:
            try:
                sections_json = sections_resp.json()
                for s in sections_json.get('parse', {}).get('sections', [])[:10]:
                    line = s.get('line')
                    if line and line.lower() not in {"references", "external links", "see also", "notes"}:
                        sections.append(line)
            except Exception:
                pass

        # 3) Fetch full plaintext extract for comparison context
        extract_url = (
            "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=1&redirects=1&format=json&titles="
            + title
        )
        full_text = ''
        extract_resp = _get_session().get(extract_url, timeout=DEFAULT_TIMEOUT)
        if extract_resp.ok:
            try:
                q = extract_resp.json()
                pages = q.get('query', {}).get('pages', {})
                if pages:
                    first = next(iter(pages.values()))
                    full_text = (first.get('extract') or '').strip()
            except Exception:
                pass

        return {
            'title': title_text,
            'intro': intro_text,
            'sections': sections[:5],
            'url': url,
            'full_text': full_text
        }
    except requests.RequestException as e:
        logger.error("Failed to fetch Wikipedia article: %s", e)
        return None


def fetch_grokipedia_article(url: str) -> Optional[dict[str, Any]]:
    """Fetch Grokipedia article using Firecrawl API.

    Uses Firecrawl to get clean markdown content from grokipedia.com.
    Falls back to SDK if Firecrawl fails.
    """
    # Try Firecrawl first for clean markdown
    firecrawl_result = scrape_with_firecrawl(url)
    if firecrawl_result and firecrawl_result.get('markdown'):
        markdown = firecrawl_result['markdown']
        title = firecrawl_result.get('title', '')

        # Clean up title - remove " | Grokipedia" suffix if present
        if ' | Grokipedia' in title:
            title = title.split(' | Grokipedia')[0].strip()
        elif ' - Grokipedia' in title:
            title = title.split(' - Grokipedia')[0].strip()

        markdown = clean_firecrawl_markdown(markdown, title=title)

        # Extract summary from first paragraph of markdown
        lines = markdown.split('\n')
        summary = ''
        for line in lines:
            line = line.strip()
            # Skip headers, empty lines, and short lines
            if line and not line.startswith('#') and len(line) > 100:
                summary = line[:500]  # First substantial paragraph
                break
        
        return {
            'title': title,
            'summary': summary,
            'sections': [],  # Firecrawl doesn't provide TOC separately
            'url': url,
            'full_text': markdown  # Clean markdown!
        }
    
    # Fallback to SDK if Firecrawl fails
    logger.debug("Firecrawl unavailable, falling back to SDK")
    if not is_sdk_available():
        logger.warning("Grokipedia SDK not available")
        return None

    try:
        # Extract slug from URL (format: https://grokipedia.com/page/Article_Name)
        parsed = urlparse(url)
        slug = parsed.path.split('/page/')[-1].strip('/')
        if not slug:
            return None

        client = get_sdk_client()
        try:
            article = client.get_article(slug)
            return {
                'title': article.title,
                'summary': article.summary,
                'sections': article.table_of_contents[:5],
                'url': str(article.url),
                'full_text': article.full_content
            }
        except ArticleNotFound:
            resolved_slug = client.find_slug(slug)
            if resolved_slug and resolved_slug != slug:
                try:
                    article = client.get_article(resolved_slug)
                    return {
                        'title': article.title,
                        'summary': article.summary,
                        'sections': article.table_of_contents[:5],
                        'url': str(article.url),
                        'full_text': article.full_content
                    }
                except (ArticleNotFound, RequestError):
                    logger.warning("Could not fetch article after resolving slug: %s", resolved_slug)
                    return None
            else:
                logger.info("Article not found in Grokipedia: %s", slug)
                return None
        except RequestError as e:
            logger.error("Error fetching Grokipedia article: %s", e)
            return None
        finally:
            client.close()
    except Exception as e:
        logger.exception("Error initializing SDK or fetching article: %s", e)
        return None

