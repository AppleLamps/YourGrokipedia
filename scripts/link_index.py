"""Local index of Grokipedia slugs based on files in ./links

Scans links/sitemap-*/names.txt which contain one slug per line (as generated by map.py),
and provides utilities to resolve an input string to an exact or closest slug.
"""

from __future__ import annotations

import os
import threading
import difflib
from functools import lru_cache
from pathlib import Path
from typing import Iterable, List, Optional, Tuple


# Thread-safe lazy load of slugs
_load_lock = threading.Lock()


def _default_links_dir() -> str:
    """Compute the default links directory relative to this file."""
    script_dir = Path(__file__).parent.parent
    links_dir = script_dir / 'grokipedia-sdk' / 'grokipedia_sdk' / 'links'
    return str(links_dir)


def _iter_names_files(base_dir: str) -> Iterable[str]:
    for root, dirs, files in os.walk(base_dir):
        if "names.txt" in files:
            yield os.path.join(root, "names.txt")


def _normalize_key(s: str) -> str:
    """Normalize strings for comparison: lowercase, replace separators, strip."""
    s = s.strip().lower()
    s = s.replace(" ", "_")
    s = s.replace("-", "_")
    return s


@lru_cache(maxsize=1)
def load_slugs(base_dir: Optional[str] = None) -> List[str]:
    """Load all slugs from links directory. Cached for the process lifetime.

    Returns a list of unique slugs (strings) in their original form.
    """
    base = base_dir or _default_links_dir()
    slugs: List[str] = []
    if not os.path.isdir(base):
        return slugs

    with _load_lock:
        seen = set()
        for path in _iter_names_files(base):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        s = line.strip()
                        if not s:
                            continue
                        if s not in seen:
                            slugs.append(s)
                            seen.add(s)
            except Exception:
                # Best-effort: skip unreadable files
                continue
    return slugs


def find_best_slug(query: str, base_dir: Optional[str] = None, cutoff: float = 0.82) -> Optional[str]:
    """Return the exact or closest slug for a given query using local index.

    - Exact match tries a few normalized variants (space/underscore/hyphen, case-insensitive)
    - Otherwise uses difflib to find the best match above cutoff similarity

    Parameters
    ----------
    query: str
        User-provided slug or name
    base_dir: Optional[str]
        Root links directory; defaults to ./links
    cutoff: float
        Similarity threshold for fuzzy match (0..1)
    """
    if not query or not isinstance(query, str):
        return None

    slugs = load_slugs(base_dir)
    if not slugs:
        return None

    # Build normalized lookup map
    norm_to_original: dict[str, str] = {}
    for s in slugs:
        norm_to_original.setdefault(_normalize_key(s), s)

    # Generate candidate keys for exact match
    q_variants = {
        query,
        query.strip(),
        query.replace(" ", "_"),
        query.replace(" ", "-"),
        query.replace("_", "-"),
        query.replace("-", "_"),
        query.title().replace(" ", "_"),
        query.title().replace(" ", "-"),
        query.lower(),
    }

    for q in list(q_variants):
        k = _normalize_key(q)
        if k in norm_to_original:
            return norm_to_original[k]

    # Fuzzy: conservative search using SequenceMatcher with additional heuristics
    qn = _normalize_key(query)
    best_key = None
    best_ratio = 0.0

    qlen = len(qn)
    qfirst = qn[0] if qn else ''
    qfirst2 = qn[:2] if len(qn) >= 2 else qn

    for cand in norm_to_original.keys():
        # Heuristics to reduce false positives
        if qfirst and cand and cand[0] != qfirst:
            continue
        if len(qn) <= 6 and qfirst2 and (cand[:2] if len(cand) >= 2 else cand) != qfirst2:
            continue
        if abs(len(cand) - qlen) > max(3, int(0.3 * max(qlen, 1))):
            continue

        r = difflib.SequenceMatcher(a=qn, b=cand, autojunk=False).ratio()
        if r > best_ratio:
            best_ratio = r
            best_key = cand

    if best_key and best_ratio >= cutoff:
        return norm_to_original.get(best_key)
    return None


def resolve_slug_or_none(query: str, base_dir: Optional[str] = None) -> Optional[str]:
    """Convenience wrapper with a conservative cutoff for user input to avoid bad matches."""
    return find_best_slug(query, base_dir=base_dir, cutoff=0.8)


if __name__ == '__main__':
    # Example usage
    test_query = "Comcast"
    result = find_best_slug(test_query)
    print(f"Query: {test_query}")
    print(f"Result: {result}")

